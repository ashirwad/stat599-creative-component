---
title: "Stroke trial modeling result"
author: "Ashirwad Barnwal"
date: "10/21/2020"
output: html_document
---

```{r before-doc, cache = FALSE}
knitr::opts_chunk$set(
  cache = TRUE, 
  autodep = TRUE
)
```

# Getting setup
Define setup chunk:

```{r setup, message = FALSE, warning = FALSE, cache = FALSE}
# Elegant handling of namespace conflicts
library(conflicted)

# Miscellaneous
library(default)
library(tictoc)
library(doFuture)
registerDoFuture()
plan(multisession)
# library(lightgbm)

# Path & data manipulation
library(here)
library(tidyverse)
conflict_prefer("filter", "dplyr")

# Model building & evaluation
library(tidymodels)
library(treesnip)
library(stacks)

# Pretty tables
library(summarytools)
st_options(
  style = "rmarkdown",
  plain.ascii = FALSE,
  footnote = NA,
  dfSummary.style = "grid",
  dfSummary.valid.col = FALSE,
  dfSummary.graph.magnif = 0.75,
  tmp.img.dir = "/img",
  subtitle.emphasis = FALSE
)

# My functions
pull_entry <- function(in_data, trt_type, col_name) {
  in_data %>%
    filter(treatment == trt_type) %>%
    pluck(col_name, 1)
}
```

```{r st-css, echo = FALSE, results = "asis"}
st_css()
```

# International Stroke Trial (IST) data

## Import data
Import IST data:

```{r ist-raw, message = FALSE, warning = FALSE}
ist_raw <- read_csv(here("data", "IST_corrected.csv"))
glimpse(ist_raw)
```

## Subset data {.tabset}
Create a subset of IST data for model building:

```{r ist}
ist <- ist_raw %>%
  select(RDELAY:STYPE, RXASP, RXHEP, FDEAD) %>%
  mutate(
    RXHEP = fct_collapse(RXHEP, "M" = c("M", "H")),
    treatment = case_when(
      RXASP == "N" & RXHEP == "N" ~ "no_asp_no_hep",
      RXASP == "N" & RXHEP == "L" ~ "no_asp_low_hep",
      RXASP == "N" & RXHEP == "M" ~ "no_asp_med_hep",
      RXASP == "Y" & RXHEP == "N" ~ "yes_asp_no_hep",
      RXASP == "Y" & RXHEP == "L" ~ "yes_asp_low_hep",
      RXASP == "Y" & RXHEP == "M" ~ "yes_asp_med_hep"
    )
  ) %>%
  select(-RXASP, -RXHEP)
glimpse(ist)
```

Create six data frames, one for each treatment group:

```{r ist-fdead}
ist_fdead <- ist %>%
  filter(FDEAD != "U") %>%
  drop_na(RATRIAL, FDEAD) %>%
  group_by(treatment) %>%
  nest() %>%
  ungroup()
glimpse(ist_fdead)
```

Generate descriptive summaries for data corresponding to each treatment group:

```{r ist-fdead-smry, message = FALSE}
ist_fdead_smry <- ist_fdead %>%
  mutate(
    df_summary = map(data, ~ dfSummary(.x))
  )
glimpse(ist_fdead_smry)
```

### Treatment A: No aspirin, no heparin
Glimpse data:

```{r ist-fdead-nasp-nhep}
pull_entry(ist_fdead, "no_asp_no_hep", "data") %>%
  glimpse()
```

View data summary:

```{r ist-fdead-nasp-nhep-smry, results = "asis"}
pull_entry(ist_fdead_smry, "no_asp_no_hep", "df_summary") %>%
  print(max.tbl.height = 600, method = "render")
```

### Treatment B: No aspirin, low heparin
Glimpse data:

```{r ist-fdead-nasp-lhep}
pull_entry(ist_fdead, "no_asp_low_hep", "data") %>%
  glimpse()
```

View data summary:

```{r ist-fdead-nasp-lhep-smry, results = "asis"}
pull_entry(ist_fdead_smry, "no_asp_low_hep", "df_summary") %>%
  print(max.tbl.height = 600, method = "render")
```

### Treatment C: No aspirin, medium heparin
Glimpse data:

```{r ist-fdead-nasp-mhep}
pull_entry(ist_fdead, "no_asp_med_hep", "data") %>%
  glimpse()
```

View data summary:

```{r ist-fdead-nasp-mhep-smry, results = "asis"}
pull_entry(ist_fdead_smry, "no_asp_med_hep", "df_summary") %>%
  print(max.tbl.height = 600, method = "render")
```

### Treatment D: aspirin, no heparin
Glimpse data:

```{r ist-fdead-asp-nhep}
pull_entry(ist_fdead, "yes_asp_no_hep", "data") %>%
  glimpse()
```

View data summary:

```{r ist-fdead-asp-nhep-smry, results = "asis"}
pull_entry(ist_fdead_smry, "yes_asp_no_hep", "df_summary") %>%
  print(max.tbl.height = 600, method = "render")
```

### Treatment E: aspirin, low heparin
Glimpse data:

```{r ist-fdead-asp-lhep}
pull_entry(ist_fdead, "yes_asp_low_hep", "data") %>%
  glimpse()
```

View data summary:

```{r ist-fdead-asp-lhep-smry, results = "asis"}
pull_entry(ist_fdead_smry, "yes_asp_low_hep", "df_summary") %>%
  print(max.tbl.height = 600, method = "render")
```

### Treatment F: aspirin, medium heparin
Glimpse data:

```{r ist-fdead-asp-mhep}
pull_entry(ist_fdead, "yes_asp_med_hep", "data") %>%
  glimpse()
```

View data summary:

```{r ist-fdead-asp-mhep-smry, results = "asis"}
pull_entry(ist_fdead_smry, "yes_asp_med_hep", "df_summary") %>%
  print(max.tbl.height = 600, method = "render")
```

## Data partitioning
Create data partitions:

```{r ist-splits}
# For reproducible results
set.seed(123)

# Data partitions
ist_fdead_nasp_nhep <- pull_entry(ist_fdead, "no_asp_no_hep", "data") %>%
  mutate(across(where(is.character), as.factor))
ist_split <- initial_split(ist_fdead_nasp_nhep, strata = FDEAD)
ist_train <- training(ist_split)
ist_test <- testing(ist_split)

ist_folds <- vfold_cv(ist_train, v = 5)
```

Create a function to partition data in nested data frames:

```{r split-ist-data}
split_ist_data <- function(data) {
  data %>%
    mutate(
      ist_split = map(data, ~ initial_split(.x, strata = FDEAD)),
      ist_train = map(ist_split, ~ training(.x)),
      ist_test = map(ist_split, ~ testing(.x)),
      ist_folds = map(ist_train, ~ vfold_cv(.x, v = 5, strata = FDEAD))
    )
}
```

## Model specification {.tabset}

### Random forest
Specify a random forest model:

```{r rf-spec}
rf_spec <- rand_forest(
  mtry = tune(),
  trees = 500,
  min_n = tune()
) %>%
  set_mode("classification") %>%
  set_engine("ranger")
rf_spec
```

### Neural network
Specify a neural network model:

```{r nnet-spec}
nnet_spec <- mlp(hidden_units = tune(), penalty = tune(), epochs = tune()) %>%
  set_mode("classification") %>%
  set_engine("nnet")
nnet_spec
```

### XGBoost
Specify an XGBoost model:

```{r xgb-spec}
xgb_spec <- boost_tree(
  mtry = tune(),
  trees = 1000,
  min_n = tune(),
  tree_depth = tune(),
  learn_rate = tune(),
  loss_reduction = tune(),
  sample_size = tune()
) %>%
  set_mode("classification") %>%
  set_engine("xgboost")
xgb_spec
```

### LightGBM
Specify a lightgbm model:

```{r lgbm-spec}
lgbm_spec <- boost_tree(
  trees = 1000,
  min_n = tune(),
  tree_depth = tune(),
) %>%
  set_mode("classification") %>%
  set_engine("lightgbm")
lgbm_spec
```

### CatBoost
Specify a catboost model:

```{r catb-spec}
catb_spec <- boost_tree(
  trees = 1000,
  min_n = tune(),
  tree_depth = tune(),
  learn_rate = tune()
) %>%
  set_mode("classification") %>%
  set_engine("catboost")
catb_spec
```

## Feature engineering {.tabset}
Define recipes for feature engineering:

### Dummy coding
Create dummy variables:

```{r base-rec}
base_rec <- recipe(FDEAD ~ ., data = ist_train) %>%
  step_dummy(all_nominal(), -all_outcomes())
base_rec
```

### Normalization
Center and scale numeric variables:

```{r norm-rec}
norm_rec <- base_rec %>%
  step_normalize(all_predictors())
norm_rec
```

### Helper
Create a helper function to add recipes to nested data frames:

```{r add-ist-recipe}
add_ist_recipe <- function(data) {
  data %>%
    mutate(
      base_rec = map(
        ist_train, 
        ~ recipe(FDEAD ~ ., data = .x) %>%
            step_dummy(all_nominal(), -all_outcomes())
      ),
      norm_rec = map(base_rec, ~ .x %>% step_normalize(all_predictors()))
    )
}
```

## Modeling workflow {.tabset}
Specify modeling workflows:

### Random forest
Specify a modeling workflow for random forest:

```{r rf-wflow}
rf_wflow <- workflow() %>%
  add_recipe(base_rec) %>%
  add_model(rf_spec)
rf_wflow
```

### Neural network
Specify a modeling workflow for neural net:

```{r nnet-wflow}
nnet_wflow <- workflow() %>%
  add_recipe(norm_rec) %>%
  add_model(nnet_spec)
nnet_wflow
```

### XGBoost
Specify a modeling workflow for XGBoost:

```{r xgb-wflow}
xgb_wflow <- workflow() %>%
  add_recipe(base_rec) %>%
  add_model(xgb_spec)
xgb_wflow
```

### LightGBM
Specify a modeling workflow for lightgbm:

```{r lgbm-wflow}
lgbm_wflow <- workflow() %>%
  add_recipe(recipe(FDEAD ~ ., data = ist_train)) %>%
  add_model(lgbm_spec)
lgbm_wflow
```

### CatBoost
Specify a modeling workflow for catboost:

```{r catb-wflow}
catb_wflow <- workflow() %>%
  add_formula(FDEAD ~ .) %>%
  add_model(catb_spec)
catb_wflow
```

### Helper
Create a helper function to add modeling workflows to nested data frames:

```{r add-ist-wflow}
add_ist_wflow <- function(data) {
  data %>%
    mutate(
      rf_wflow = map2(
        base_rec, rf_spec, ~ workflow() %>% add_recipe(.x) %>% add_model(.y)
      ),
      nnet_wflow = map2(
        norm_rec, nnet_spec, ~ workflow() %>% add_recipe(.x) %>% add_model(.y)
      ),
      xgb_wflow = map2(
        base_rec, xgb_spec, ~ workflow() %>% add_recipe(.x) %>% add_model(.y)
      )
    )
}
```

## Model tuning {.tabset}
Define control settings for data stacking:

```{r cntrl-grid}
cntrl_grid <- control_stack_grid()
cntrl_grid
```

### Random forest
Tune random forest model:

```{r rf-res}
tic()
set.seed(345)
rf_res <- tune_grid(
  object = rf_wflow,
  resamples = ist_folds,
  grid = 10,
  control = cntrl_grid
)
toc()
rf_res
```

Define a function to tune random forest model for nested data frames:

```{r tune-ist-rf}
tune_ist_rf <- function(data, cntrl_grid) {
  data %>%
    mutate(
      rf_res = map2(
        rf_wflow, 
        ist_folds, 
        ~ tune_grid(
            object = .x,
            resamples = .y,
            grid = 10,
            control = cntrl_grid
          )
      )
    )
}
```

### Neural network
Tune neural net model:

```{r nnet-res}
tic()
set.seed(123)
nnet_res <- tune_grid(
  object = nnet_wflow,
  resamples = ist_folds,
  grid = 10,
  control = cntrl_grid
)
toc()
nnet_res
```

Define a function to tune neural network model for nested data frames:

```{r tune-ist-nnet}
tune_ist_nnet <- function(data, cntrl_grid) {
  data %>%
    mutate(
      nnet_res = map2(
        nnet_wflow, 
        ist_folds, 
        ~ tune_grid(
            object = .x,
            resamples = .y,
            grid = 10,
            control = cntrl_grid
          )
      )
    )
}
```

### XGBoost
Construct parameter grid:

```{r xgb-grid}
xgb_grid <- grid_latin_hypercube(
  min_n(),
  tree_depth(),
  learn_rate(),
  loss_reduction(),
  sample_size = sample_prop(),
  finalize(mtry(), ist_train),
  size = 30
)
xgb_grid
```

Tune XGBoost model:

```{r xgb-res}
tic()
set.seed(123)
xgb_res <- tune_grid(
  object = xgb_wflow,
  resamples = ist_folds,
  grid = xgb_grid,
  control = cntrl_grid
)
toc()
xgb_res
```

Define a function to tune xgboost model for nested data frames:

```{r tune-ist-xgb}
tune_ist_xgb <- function(data, cntrl_grid) {
  data %>%
    mutate(
      xgb_grid = map(
        ist_train,
        ~ grid_latin_hypercube(
            min_n(),
            tree_depth(),
            learn_rate(),
            loss_reduction(),
            sample_size = sample_prop(),
            finalize(mtry(), .x),
            size = 30
          )
      ),
      xgb_res = pmap(
        list(xgb_wflow, ist_folds, xgb_grid),
        ~ tune_grid(
            object = ..1,
            resamples = ..2,
            grid = ..3,
            control = cntrl_grid
          )
      )
    )
}
```

### LightGBM
Construct parameter grid:

```{r lgbm-grid}
lgbm_grid <- grid_latin_hypercube(
  min_n(),
  tree_depth(),
  size = 30
)
lgbm_grid
```

Tune lightgbm model:

```{r lgbm-res, eval = FALSE}
# TODO: Current version of lightgbm crashes R session;
# run this chunk when new version is available on cran
tic()
set.seed(123)
lgbm_res <- tune_grid(
  object = lgbm_wflow,
  resamples = ist_folds,
  grid = lgbm_grid,
  control = cntrl_grid
)
toc()
lgbm_res
```

### CatBoost
Construct parameter grid:

```{r catb-grid}
catb_grid <- grid_latin_hypercube(
  min_n(),
  tree_depth(range = c(4, 10)),
  learn_rate(),
  size = 30
)
catb_grid
```

Tune catboost model:

```{r catb-res}
tic()
set.seed(123)
catb_res <- tune_grid(
  object = catb_wflow,
  resamples = ist_folds,
  grid = catb_grid,
  control = cntrl_grid
)
toc()
catb_res
```

## Best models {.tabset}
Identify best models and create final model specifications:

### Random forest
Best random forest model:

```{r rf-final-spec}
rf_best_auc <- select_best(rf_res, "roc_auc")
rf_final_spec <- finalize_model(rf_spec, rf_best_auc)
rf_final_spec
```

### Neural network
Best neural net model:

```{r nnet-final-spec}
nnet_best_auc <- select_best(nnet_res, "roc_auc")
nnet_final_spec <- finalize_model(nnet_spec, nnet_best_auc)
nnet_final_spec
```

### XGBoost
Best XGBoost model:

```{r xgb-final-spec}
xgb_best_auc <- select_best(xgb_res, "roc_auc")
xgb_final_spec <- finalize_model(xgb_spec, xgb_best_auc)
xgb_final_spec
```

### CatBoost
Best catboost model:

```{r catb-final-spec}
catb_best_auc <- select_best(catb_res, "roc_auc")
catb_final_spec <- finalize_model(catb_spec, catb_best_auc)
catb_final_spec
```

## Final workflows {.tabset}
Specify final workflows:

### Random forest
Create a final workflow for fitting random forest:

```{r rf-final-wflow}
rf_final_wflow <- workflow() %>%
  add_recipe(base_rec) %>%
  add_model(rf_final_spec)
rf_final_wflow
```

### Neural network
Create a final workflow for fitting neural network:

```{r nnet-final-wflow}
nnet_final_wflow <- workflow() %>%
  add_recipe(norm_rec) %>%
  add_model(nnet_final_spec)
nnet_final_wflow
```

### XGBoost
Create a final workflow for fitting XGBoost:

```{r xgb-final-wflow}
xgb_final_wflow <- workflow() %>%
  add_recipe(base_rec) %>%
  add_model(xgb_final_spec)
xgb_final_wflow
```

### CatBoost
Create a final workflow for fitting catboost:

```{r catb-final-wflow}
catb_final_wflow <- workflow() %>%
  add_formula(FDEAD ~ .) %>%
  add_model(catb_final_spec)
catb_final_wflow
```

## Final fits {.tabset}
Fit final best models to the training set and evaluate the test set:

### Random forest
Fit the final best random forest model to the training set and evaluate the test set:

```{r rf-final-res}
tic()
rf_final_res <- rf_final_wflow %>%
  last_fit(ist_split)
toc()

rf_final_res %>%
  collect_metrics()
```

### Neural network
Fit the final best neural network model to the training set and evaluate the test set:

```{r nnet-final-res}
tic()
nnet_final_res <- nnet_final_wflow %>%
  last_fit(ist_split)
toc()

nnet_final_res %>%
  collect_metrics()
```

### XGBoost
Fit the final best xgboost model to the training set and evaluate the test set:

```{r xgb-final-res}
tic()
xgb_final_res <- xgb_final_wflow %>%
  last_fit(ist_split)
toc()

xgb_final_res %>%
  collect_metrics()
```

## ROC curve {.tabset}

### Random forest
Compute the data needed to plot the ROC curve for random forest model:

```{r rf-auc}
rf_auc <- rf_final_res %>%
  collect_predictions() %>%
  roc_curve(FDEAD, .pred_Y, event_level = "second") %>%
  mutate(model = "Random forest")
rf_auc
```

### Neural network
Compute the data needed to plot the ROC curve for neural network model:

```{r nnet-auc}
nnet_auc <- nnet_final_res %>%
  collect_predictions() %>%
  roc_curve(FDEAD, .pred_Y, event_level = "second") %>%
  mutate(model = "Neural network")
nnet_auc
```

### XGBoost
Compute the data needed to plot the ROC curve for xgboost model:

```{r xgb-auc}
xgb_auc <- xgb_final_res %>%
  collect_predictions() %>%
  roc_curve(FDEAD, .pred_Y, event_level = "second") %>%
  mutate(model = "XGBoost")
xgb_auc
```

## {-}
Compare ROC curves:

```{r roc-curves}
roc_curves <- bind_rows(rf_auc, nnet_auc, xgb_auc) %>%
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  geom_path(lwd = 1.5, alpha = 0.8) + 
  geom_abline(lty = 3) + 
  coord_equal() + 
  scale_color_viridis_d(option = "plasma", end = 0.6) +
  ggplotif
roc_curves
```





