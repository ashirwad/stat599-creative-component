---
title: "Stroke trial modeling result"
author: "Ashirwad Barnwal"
date: "10/21/2020"
output: html_document
---

```{r before-doc, cache = FALSE}
knitr::opts_chunk$set(
  cache = TRUE, 
  autodep = TRUE
)
```

# Getting setup
Define setup chunk:

```{r setup, message = FALSE, warning = FALSE, cache = FALSE}
# Elegant handling of namespace conflicts
library(conflicted)

# Miscellaneous
library(tictoc)
library(doFuture)
registerDoFuture()
plan(multisession)

# Path & data manipulation
library(here)
library(tidyverse)
conflict_prefer("filter", "dplyr")
library(rio)
conflict_prefer("export", "rio")

# Pretty plots
library(ggpubr)
library(plotly)
conflict_prefer("layout", "plotly")

# Model building & evaluation
library(tidymodels)

# Model explanation
library(DALEX)
library(DALEXtra)

# Set options
options(datatable.na.strings = c("", "NA")) # read these strings as NA
```

# International Stroke Trial (IST) data
Import IST data:

```{r ist}
ist <- import(here("output", "rds-files", "ist.rds"))
glimpse(ist)
```

# Data partitioning
Create data partitions:

```{r ist-splits}
# For reproducible results
set.seed(123)

# Data partitions
ist_split <- initial_split(sample_frac(ist, 0.1), strata = dead_or_dep)
ist_train <- training(ist_split)
ist_test <- testing(ist_split)

ist_folds <- vfold_cv(ist_train, v = 5)
```

## Model specification {.tabset}

### Random forest
Specify a random forest model:

```{r rf-spec}
rf_spec <- rand_forest(
  mtry = tune(),
  trees = 500,
  min_n = tune()
) %>%
  set_mode("classification") %>%
  set_engine("ranger")
rf_spec
```

### Neural network
Specify a neural network model:

```{r nnet-spec}
nnet_spec <- mlp(hidden_units = tune(), penalty = tune(), epochs = tune()) %>%
  set_mode("classification") %>%
  set_engine("nnet")
nnet_spec
```

### XGBoost
Specify an XGBoost model:

```{r xgb-spec}
xgb_spec <- boost_tree(
  mtry = tune(),
  trees = 1000,
  min_n = tune(),
  tree_depth = tune(),
  learn_rate = tune(),
  loss_reduction = tune(),
  sample_size = tune()
) %>%
  set_mode("classification") %>%
  set_engine("xgboost")
xgb_spec
```

## Feature engineering {.tabset}
Define recipes for feature engineering:

### Dummy coding
Create dummy variables:

```{r base-rec}
base_rec <- recipe(dead_or_dep ~ ., data = ist_train) %>%
  step_dummy(all_nominal(), -all_outcomes())
base_rec
```

### Normalization
Center and scale numeric variables:

```{r norm-rec}
norm_rec <- base_rec %>%
  step_normalize(all_predictors())
norm_rec
```

## Modeling workflow {.tabset}
Specify modeling workflows:

### Random forest
Specify a modeling workflow for random forest:

```{r rf-wflow}
rf_wflow <- workflow() %>%
  add_recipe(base_rec) %>%
  add_model(rf_spec)
rf_wflow
```

### Neural network
Specify a modeling workflow for neural net:

```{r nnet-wflow}
nnet_wflow <- workflow() %>%
  add_recipe(norm_rec) %>%
  add_model(nnet_spec)
nnet_wflow
```

### XGBoost
Specify a modeling workflow for XGBoost:

```{r xgb-wflow}
xgb_wflow <- workflow() %>%
  add_recipe(base_rec) %>%
  add_model(xgb_spec)
xgb_wflow
```

## Model tuning {.tabset}

### Random forest
Tune random forest model:

```{r rf-res}
tic()
set.seed(345)
rf_res <- tune_grid(
  object = rf_wflow,
  resamples = ist_folds,
  grid = 10
)
toc()
rf_res
```

### Neural network
Tune neural net model:

```{r nnet-res}
tic()
set.seed(123)
nnet_res <- tune_grid(
  object = nnet_wflow,
  resamples = ist_folds,
  grid = 10
)
toc()
nnet_res
```

### XGBoost
Construct parameter grid:

```{r xgb-grid}
xgb_grid <- grid_latin_hypercube(
  min_n(),
  tree_depth(),
  learn_rate(),
  loss_reduction(),
  sample_size = sample_prop(),
  finalize(mtry(), ist_train),
  size = 30
)
xgb_grid
```

Tune XGBoost model:

```{r xgb-res}
tic()
set.seed(123)
xgb_res <- tune_grid(
  object = xgb_wflow,
  resamples = ist_folds,
  grid = xgb_grid
)
toc()
xgb_res
```

## Best models {.tabset}
Identify best models and create final model specifications:

### Random forest
Best random forest model:

```{r rf-final-spec}
rf_best_acc <- select_best(rf_res, "accuracy")
rf_final_spec <- finalize_model(rf_spec, rf_best_acc)
rf_final_spec
```

### Neural network
Best neural net model:

```{r nnet-final-spec}
nnet_best_acc <- select_best(nnet_res, "accuracy")
nnet_final_spec <- finalize_model(nnet_spec, nnet_best_acc)
nnet_final_spec
```

### XGBoost
Best XGBoost model:

```{r xgb-final-spec}
xgb_best_acc <- select_best(xgb_res, "accuracy")
xgb_final_spec <- finalize_model(xgb_spec, xgb_best_acc)
xgb_final_spec
```

## Final workflows {.tabset}
Specify final workflows:

### Random forest
Create a final workflow for fitting random forest:

```{r rf-final-wflow}
rf_final_wflow <- workflow() %>%
  add_recipe(base_rec) %>%
  add_model(rf_final_spec)
rf_final_wflow
```

### Neural network
Create a final workflow for fitting neural network:

```{r nnet-final-wflow}
nnet_final_wflow <- workflow() %>%
  add_recipe(norm_rec) %>%
  add_model(nnet_final_spec)
nnet_final_wflow
```

### XGBoost
Create a final workflow for fitting XGBoost:

```{r xgb-final-wflow}
xgb_final_wflow <- workflow() %>%
  add_recipe(base_rec) %>%
  add_model(xgb_final_spec)
xgb_final_wflow
```

## Final fits {.tabset}
Fit final best models to the training set and evaluate the test set:

### Random forest
Fit the final best random forest model to the training set and evaluate the test set:

```{r rf-final-res}
tic()
rf_final_res <- rf_final_wflow %>%
  last_fit(ist_split)
toc()

rf_final_res %>%
  collect_metrics()
```

### Neural network
Fit the final best neural network model to the training set and evaluate the test set:

```{r nnet-final-res}
tic()
nnet_final_res <- nnet_final_wflow %>%
  last_fit(ist_split)
toc()

nnet_final_res %>%
  collect_metrics()
```

### XGBoost
Fit the final best xgboost model to the training set and evaluate the test set:

```{r xgb-final-res}
tic()
xgb_final_res <- xgb_final_wflow %>%
  last_fit(ist_split)
toc()

xgb_final_res %>%
  collect_metrics()
```

## ROC curve {.tabset}

### Random forest
Compute the data needed to plot the ROC curve for random forest model:

```{r rf-auc}
rf_auc <- rf_final_res %>%
  collect_predictions() %>%
  roc_curve(dead_or_dep, .pred_yes, event_level = "second") %>%
  mutate(model = "Random forest")
rf_auc
```

### Neural network
Compute the data needed to plot the ROC curve for neural network model:

```{r nnet-auc}
nnet_auc <- nnet_final_res %>%
  collect_predictions() %>%
  roc_curve(dead_or_dep, .pred_yes, event_level = "second") %>%
  mutate(model = "Neural network")
nnet_auc
```

### XGBoost
Compute the data needed to plot the ROC curve for xgboost model:

```{r xgb-auc}
xgb_auc <- xgb_final_res %>%
  collect_predictions() %>%
  roc_curve(dead_or_dep, .pred_yes, event_level = "second") %>%
  mutate(model = "XGBoost")
xgb_auc
```

## {-}
Compare ROC curves:

```{r roc-curves}
roc_curves <- bind_rows(rf_auc, nnet_auc, xgb_auc) %>%
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  geom_path(lwd = 1.5, alpha = 0.8) + 
  geom_abline(lty = 3) + 
  coord_equal() + 
  scale_color_manual(values = colors_discrete_drwhy(3)) + 
  theme_pubclean()
roc_curves
```

```{r roc-curves2}
roc_curves2 <- ggplotly(roc_curves) %>%
  layout(legend = list(orientation = "h", xanchor = "center", x = 0.5, y = 1))
roc_curves2
```

```{r export-roc-curves2, include = FALSE}
export(roc_curves2, here("output", "rds-files", "roc-curves.rds"))
```

# Model interpretation

## Define explainers

### Random forest
Create explainer for random forest:

```{r explainer-rf}
explainer_rf <- explain_tidymodels(
  rf_final_res$.workflow[[1]],
  data = select(ist_train, -dead_or_dep),
  y = as.numeric(pull(ist_train, dead_or_dep)) - 1,
  verbose = FALSE,
  label = "RF"
)
explainer_rf
```

### Neural network
Create explainer for neural network:

```{r explainer-nnet}
explainer_nnet <- explain_tidymodels(
  nnet_final_res$.workflow[[1]],
  data = select(ist_train, -dead_or_dep),
  y = as.numeric(pull(ist_train, dead_or_dep)) - 1,
  verbose = FALSE,
  label = "NNet"
)
explainer_nnet
```

### XGBoost
Create explainer for XGBoost:

```{r explainer-xgb}
explainer_xgb <- explain_tidymodels(
  xgb_final_res$.workflow[[1]],
  data = select(ist_train, -dead_or_dep),
  y = as.numeric(pull(ist_train, dead_or_dep)) - 1,
  verbose = FALSE,
  label = "XGB"
)
explainer_xgb
```

## Variable importance

### Random forest
Compute variable importance for random forest:

```{r vip-rf}
vip_rf <- model_parts(explainer_rf, loss_function = loss_one_minus_auc)
vip_rf
```

### Neural network
Compute variable importance for neural network:

```{r vip-nnet}
vip_nnet <- model_parts(explainer_nnet, loss_function = loss_one_minus_auc)
vip_nnet
```

### XGBoost
Compute variable importance for XGBoost:

```{r vip-xgb}
vip_xgb <- model_parts(explainer_xgb, loss_function = loss_one_minus_auc)
vip_xgb
```

Create variable importance plot:

```{r vip-plot}
plot_vip(vip_rf, vip_nnet, vip_xgb)
```

## Local interpretation

### Random forest
Compute SHAP attributions:

```{r local-rf}
local_rf <- predict_parts(
  explainer_rf, explainer_rf$data[1, ], type = "shap", B = 20
)
```

Create a plot of SHAP attributions:

```{r local-rf-plot}
local_rf_plot <- local_rf %>%
  with_groups(variable, ~ mutate(.x, mean_val = mean(contribution))) %>%
  mutate(variable = fct_reorder(variable, abs(mean_val))) %>%
  ggplot(aes(contribution, variable, fill = mean_val > 0)) +
  geom_col(
    data = ~ distinct(., variable, mean_val), 
    aes(mean_val, variable), 
    alpha = 0.5
  ) +
  geom_boxplot(width = 0.5) +
  scale_fill_manual(values = colors_discrete_drwhy(2)) +
  labs(y = NULL) + 
  ggpubr::theme_pubclean() +
  theme(legend.position = "none")
local_rf_plot
```


