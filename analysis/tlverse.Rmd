---
title: "Stroke trial modeling result"
author: "Ashirwad Barnwal"
date: "10/21/2020"
output: html_document
---

```{r before-doc, cache = FALSE}
knitr::opts_chunk$set(
  cache = TRUE, 
  autodep = TRUE
)
```

# Getting setup
Define setup chunk:

```{r setup, message = FALSE, warning = FALSE, cache = FALSE}
# Elegant handling of namespace conflicts
library(conflicted)

# Miscellaneous
library(tictoc)
library(doFuture)
registerDoFuture()
plan(multisession)

# Path & data manipulation
library(here)
library(tidyverse)
conflict_prefer("filter", "dplyr")

# Model building & evaluation
library(tidymodels)
```

# International Stroke Trial (IST) data

## Import data
Import IST data:

```{r ist-raw, message = FALSE, warning = FALSE}
ist_raw <- read_csv(here("data", "IST_corrected.csv"))
glimpse(ist_raw)
```

## Subset data {.tabset}
Create a subset of IST data for model building:

```{r ist}
ist <- ist_raw %>%
  select(RDELAY:STYPE, RXASP, RXHEP, OCCODE) %>%
  mutate(
    RXHEP = fct_collapse(RXHEP, "M" = c("M", "H")),
    treatment = case_when(
      RXASP == "N" & RXHEP == "N" ~ "no_asp_no_hep",
      RXASP == "N" & RXHEP == "L" ~ "no_asp_low_hep",
      RXASP == "N" & RXHEP == "M" ~ "no_asp_med_hep",
      RXASP == "Y" & RXHEP == "N" ~ "yes_asp_no_hep",
      RXASP == "Y" & RXHEP == "L" ~ "yes_asp_low_hep",
      RXASP == "Y" & RXHEP == "M" ~ "yes_asp_med_hep"
    ),
    treatment = fct_relevel(treatment, "no_asp_no_hep") %>%
      fct_relevel("yes_asp_no_hep", after = 3),
    OCCODE = recode(
      OCCODE, `1` = "dead", `2` = "dependent", `3` = "not_recovered", 
      `4` = "recovered", .default = "missing"
    ),
    six_month_outcome = fct_relevel(OCCODE, rev) %>% 
      fct_relevel("missing", after = Inf)
  ) %>%
  select(-RXASP, -RXHEP, -OCCODE)
glimpse(ist)
```

## Data partitioning
Create data partitions:

```{r ist-splits}
# For reproducible results
set.seed(123)

# Data partitions
ist_dependent <- ist %>%
  filter(six_month_outcome != "missing") %>%
  mutate(
    six_month_outcome = fct_drop(six_month_outcome, "missing"),
    dependent = if_else(six_month_outcome == "dependent", "yes", "no") %>%
      fct_relevel("no")
  ) %>%
  select(-six_month_outcome) %>%
  drop_na() %>%
  sample_frac(0.15)

ist_split <- initial_split(ist_dependent, strata = dependent)
ist_train <- training(ist_split)
ist_test <- testing(ist_split)

ist_folds <- vfold_cv(ist_train, v = 5)
```

Create a function to partition data in nested data frames:

```{r split-ist-data}
split_ist_data <- function(data) {
  data %>%
    mutate(
      ist_split = map(data, ~ initial_split(.x, strata = FDEAD)),
      ist_train = map(ist_split, ~ training(.x)),
      ist_test = map(ist_split, ~ testing(.x)),
      ist_folds = map(ist_train, ~ vfold_cv(.x, v = 5, strata = FDEAD))
    )
}
```

## Model specification {.tabset}

### Random forest
Specify a random forest model:

```{r rf-spec}
rf_spec <- rand_forest(
  mtry = tune(),
  trees = 500,
  min_n = tune()
) %>%
  set_mode("classification") %>%
  set_engine("ranger")
rf_spec
```

### Neural network
Specify a neural network model:

```{r nnet-spec}
nnet_spec <- mlp(hidden_units = tune(), penalty = tune(), epochs = tune()) %>%
  set_mode("classification") %>%
  set_engine("nnet")
nnet_spec
```

### XGBoost
Specify an XGBoost model:

```{r xgb-spec}
xgb_spec <- boost_tree(
  mtry = tune(),
  trees = 1000,
  min_n = tune(),
  tree_depth = tune(),
  learn_rate = tune(),
  loss_reduction = tune(),
  sample_size = tune()
) %>%
  set_mode("classification") %>%
  set_engine("xgboost")
xgb_spec
```

## Feature engineering {.tabset}
Define recipes for feature engineering:

### Dummy coding
Create dummy variables:

```{r base-rec}
base_rec <- recipe(dependent ~ ., data = ist_train) %>%
  step_dummy(all_nominal(), -all_outcomes())
base_rec
```

### Normalization
Center and scale numeric variables:

```{r norm-rec}
norm_rec <- base_rec %>%
  step_normalize(all_predictors())
norm_rec
```

## Modeling workflow {.tabset}
Specify modeling workflows:

### Random forest
Specify a modeling workflow for random forest:

```{r rf-wflow}
rf_wflow <- workflow() %>%
  add_recipe(base_rec) %>%
  add_model(rf_spec)
rf_wflow
```

### Neural network
Specify a modeling workflow for neural net:

```{r nnet-wflow}
nnet_wflow <- workflow() %>%
  add_recipe(norm_rec) %>%
  add_model(nnet_spec)
nnet_wflow
```

### XGBoost
Specify a modeling workflow for XGBoost:

```{r xgb-wflow}
xgb_wflow <- workflow() %>%
  add_recipe(base_rec) %>%
  add_model(xgb_spec)
xgb_wflow
```

## Model tuning {.tabset}

### Random forest
Tune random forest model:

```{r rf-res}
tic()
set.seed(345)
rf_res <- tune_grid(
  object = rf_wflow,
  resamples = ist_folds,
  grid = 10
)
toc()
rf_res
```

### Neural network
Tune neural net model:

```{r nnet-res}
tic()
set.seed(123)
nnet_res <- tune_grid(
  object = nnet_wflow,
  resamples = ist_folds,
  grid = 10
)
toc()
nnet_res
```

### XGBoost
Construct parameter grid:

```{r xgb-grid}
xgb_grid <- grid_latin_hypercube(
  min_n(),
  tree_depth(),
  learn_rate(),
  loss_reduction(),
  sample_size = sample_prop(),
  finalize(mtry(), ist_train),
  size = 30
)
xgb_grid
```

Tune XGBoost model:

```{r xgb-res}
tic()
set.seed(123)
xgb_res <- tune_grid(
  object = xgb_wflow,
  resamples = ist_folds,
  grid = xgb_grid
)
toc()
xgb_res
```

## Best models {.tabset}
Identify best models and create final model specifications:

### Random forest
Best random forest model:

```{r rf-final-spec}
rf_best_acc <- select_best(rf_res, "accuracy")
rf_final_spec <- finalize_model(rf_spec, rf_best_acc)
rf_final_spec
```

### Neural network
Best neural net model:

```{r nnet-final-spec}
nnet_best_acc <- select_best(nnet_res, "accuracy")
nnet_final_spec <- finalize_model(nnet_spec, nnet_best_acc)
nnet_final_spec
```

### XGBoost
Best XGBoost model:

```{r xgb-final-spec}
xgb_best_acc <- select_best(xgb_res, "accuracy")
xgb_final_spec <- finalize_model(xgb_spec, xgb_best_acc)
xgb_final_spec
```

## Final workflows {.tabset}
Specify final workflows:

### Random forest
Create a final workflow for fitting random forest:

```{r rf-final-wflow}
rf_final_wflow <- workflow() %>%
  add_recipe(base_rec) %>%
  add_model(rf_final_spec)
rf_final_wflow
```

### Neural network
Create a final workflow for fitting neural network:

```{r nnet-final-wflow}
nnet_final_wflow <- workflow() %>%
  add_recipe(norm_rec) %>%
  add_model(nnet_final_spec)
nnet_final_wflow
```

### XGBoost
Create a final workflow for fitting XGBoost:

```{r xgb-final-wflow}
xgb_final_wflow <- workflow() %>%
  add_recipe(base_rec) %>%
  add_model(xgb_final_spec)
xgb_final_wflow
```

## Final fits {.tabset}
Fit final best models to the training set and evaluate the test set:

### Random forest
Fit the final best random forest model to the training set and evaluate the test set:

```{r rf-final-res}
tic()
rf_final_res <- rf_final_wflow %>%
  last_fit(ist_split)
toc()

rf_final_res %>%
  collect_metrics()
```

### Neural network
Fit the final best neural network model to the training set and evaluate the test set:

```{r nnet-final-res}
tic()
nnet_final_res <- nnet_final_wflow %>%
  last_fit(ist_split)
toc()

nnet_final_res %>%
  collect_metrics()
```

### XGBoost
Fit the final best xgboost model to the training set and evaluate the test set:

```{r xgb-final-res}
tic()
xgb_final_res <- xgb_final_wflow %>%
  last_fit(ist_split)
toc()

xgb_final_res %>%
  collect_metrics()
```

## ROC curve {.tabset}

### Random forest
Compute the data needed to plot the ROC curve for random forest model:

```{r rf-auc}
rf_auc <- rf_final_res %>%
  collect_predictions() %>%
  roc_curve(dependent, .pred_yes, event_level = "second") %>%
  mutate(model = "Random forest")
rf_auc
```

### Neural network
Compute the data needed to plot the ROC curve for neural network model:

```{r nnet-auc}
nnet_auc <- nnet_final_res %>%
  collect_predictions() %>%
  roc_curve(dependent, .pred_yes, event_level = "second") %>%
  mutate(model = "Neural network")
nnet_auc
```

### XGBoost
Compute the data needed to plot the ROC curve for xgboost model:

```{r xgb-auc}
xgb_auc <- xgb_final_res %>%
  collect_predictions() %>%
  roc_curve(dependent, .pred_yes, event_level = "second") %>%
  mutate(model = "XGBoost")
xgb_auc
```

## {-}
Compare ROC curves:

```{r roc-curves}
roc_curves <- bind_rows(rf_auc, nnet_auc, xgb_auc) %>%
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  geom_path(lwd = 1.5, alpha = 0.8) + 
  geom_abline(lty = 3) + 
  coord_equal() + 
  scale_color_viridis_d(option = "plasma", end = 0.6)
roc_curves
```





